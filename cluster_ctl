#!/usr/bin/env python3

import json, math, os, shutil, subprocess, sys, threading, time
from pathlib import Path
from queue import Queue, Empty
from string import Template
from functools import cache

# Constants
USER = "ubuntu"
WORKSPACE = f"/home/{USER}/workspace"
RUN_DIR = f"{WORKSPACE}/rondb-run"
NUM_CLUSTER_CONN_RDRS=2
NUM_CLUSTER_CONN_BENCH=1

# Node types and coloring
node_types = [
    ("ndb_mgmd",  "48;2;0;120;0;38;5;255"),
    ("ndbmtd",    "48;2;0;214;0;38;5;16"),
    ("mysqld",    "48;2;104;62;254;38;5;255"),
    ("rdrs",      "48;2;200;169;254;38;5;16"),
    ("prometheus","48;2;184;0;184;38;5;255"),
    ("grafana",   "48;2;254;144;249;38;5;16"),
    ("bench",     "48;2;190;190;0;38;5;16"),
]
import sys
stdout_is_tty=sys.stdout.isatty()
def color(s, c, b=""): return f"\033[{c}m{b+s+b}\033[0m" if stdout_is_tty else s
def red(s): return color(s, '1;31')
def green(s): return color(s, '1;32')
def lnk(uri):
    if not stdout_is_tty: return uri
    # Make URI bold underlined, and if supported, clickable.
    return f"\033]8;;{uri}\033\\\033[1;4m{uri}\033[0m\033]8;;\033\\"

# Nodes
@cache
def all_nodes():
    nodes = []
    for node_type, clr in node_types:
        public_ips = get_tf_output(f'{node_type}_public_ips')
        private_ips = get_tf_output(f'{node_type}_private_ips')
        assert len(public_ips) == len(private_ips), "IP count mismatch"
        for i in range(len(public_ips)):
            nodes.append(Node(node_type, i + 1, public_ips[i], private_ips[i], clr))
    return nodes
@cache
def max_node_name_len(): return max(len(n.name()) for n in all_nodes())
def nodes_of_type(node_type):
    return [n for n in all_nodes() if n.type == node_type]
def node_count(node_type): return len(nodes_of_type(node_type))
class Node:
    def __init__(self, type, idx, public_ip, private_ip, clr):
        self.type = type
        self.idx = idx
        self.public_ip = public_ip
        self.private_ip = private_ip
        self.clr = clr
        self.ndb_nodeids = []
    def name(self):
        ret = self.type
        if node_count(self.type) > 1:
            ret += f"_{self.idx}"
        return ret
    def tui_name(self):
        name = self.name()
        suffix = ' ' * (max_node_name_len() - len(name))
        return color(name, self.clr, " ") + suffix
def first(node_type):
    for node in all_nodes():
        if node.type == node_type:
            return node
    raise ValueError(f"No node of type {node_type} found")

# Terraform outputs
@cache
def tf_output():
    tf_output_file = Path("tf_output")
    state_file = Path("terraform.tfstate")
    if not state_file.exists():
        die("Error: terraform.tfstate does not exist."+
            " Perhaps you forgot to run 'terraform apply'?")
    if not (tf_output_file.exists() and
            state_file.exists() and
            state_file.stat().st_mtime < tf_output_file.stat().st_mtime):
        proc = subprocess.run("terraform output -json > tf_output", shell=True,
                              check=True, capture_output=True, text=True)
        if proc.returncode != 0:
            die(f"Error running terraform output: {proc.stdout + proc.stderr}")
    return json.loads(tf_output_file.read_text())
def get_tf_output(output_name):
    return tf_output()[output_name]['value']

# Generate ./config_files
def generate_config_files():
    # Util functions
    def w(name, *content): Path(name).write_text("\n".join(content) + "\n")
    def mkdir(p): Path(p).mkdir(parents=True, exist_ok=True)
    # Clean directory
    shutil.rmtree("config_files", ignore_errors=True)
    mkdir("config_files")
    # Extract some values
    USER="ubuntu"
    WORKSPACE=f"/home/{USER}/workspace"
    CONFIG_FILES=f"/home/{USER}/config_files"
    RUN_DIR=f"{WORKSPACE}/rondb-run"
    NDB_MGMD_PRI = first('ndb_mgmd').private_ip
    NO_OF_REPLICAS = get_tf_output('rondb_replicas')
    NUM_AZS = get_tf_output('num_azs')
    RONDB_VERSION = get_tf_output('rondb_version')
    GLIBC_VERSION = get_tf_output('glibc_version')
    CPU_PLATFORM = get_tf_output('cpu_platform')
    TARBALL_NAME = f"rondb-{RONDB_VERSION}-linux-glibc{GLIBC_VERSION}-{CPU_PLATFORM}.tar.gz"
    # Generate shell_vars
    w("config_files/shell_vars",
      f"USER={USER}",
      f"WORKSPACE={WORKSPACE}",
      f"CONFIG_FILES={CONFIG_FILES}",
      f"RUN_DIR={RUN_DIR}",
      f"TARBALL_NAME={TARBALL_NAME}",
      f"NUM_AZS={NUM_AZS}",
      f"NDB_MGMD_PRI={NDB_MGMD_PRI}",
      f"MYSQLD_PUB_1={first('mysqld').public_ip}",
      f"RDRS_LB=http://{get_tf_output('rdrs_nlb_dns')}:4406",
      f"RONDIS_LB={get_tf_output('rondis_nlb_dns')}:6379",
      f"RDRS_PRI_1={first('rdrs').private_ip}",
      f"BENCH_PRI_1={first('bench').private_ip}",
      )
    # Generate config.ini and populate Node.ndb_nodeids
    configini = [
      "[NDBD DEFAULT]",
      "AutomaticThreadConfig=true",
      "AutomaticMemoryConfig=true",
      "MaxDMLOperationsPerTransaction=100000",
      "MaxNoOfConcurrentOperations=100000",
      "",
      f"NoOfReplicas={NO_OF_REPLICAS}",
      "PartitionsPerNode=4",
      "",
      "[NDB_MGMD]",
      f"HostName={NDB_MGMD_PRI}",
      f"DataDir={RUN_DIR}/ndb_mgmd/data",
    ]
    last_node_id = 1
    def next_node_id(n):
        nonlocal last_node_id
        last_node_id += 1
        n.ndb_nodeids += [last_node_id]
        return last_node_id
    for node in all_nodes():
        configini += [""]
        if node.type == "ndbmtd":
            configini += [
                "[NDBD]",
                f"HostName={node.private_ip}",
                f"NodeId={next_node_id(node)}",
                f"DataDir={RUN_DIR}/{node.type}/data",
                f"FileSystemPath={RUN_DIR}/ndbmtd/ndb_data",
                f"FileSystemPathDD={RUN_DIR}/ndbmtd/ndb_disk_columns",
                f"LocationDomainId={node.idx % NUM_AZS}"
            ]
        if node.type == "mysqld":
            configini += [
                "[MYSQLD]",
                f"HostName={node.private_ip}",
                f"NodeId={next_node_id(node)}",
                f"LocationDomainId={node.idx % NUM_AZS}"
            ]
        if node.type == "rdrs":
            for _ in range(NUM_CLUSTER_CONN_RDRS):
                configini += [
                    "[API]",
                    "# RDRS",
                    f"HostName={node.private_ip}",
                    f"NodeId={next_node_id(node)}",
                    f"LocationDomainId={node.idx % NUM_AZS}"
                ]
        if node.type == "bench":
            for _ in range(NUM_CLUSTER_CONN_BENCH):
                configini += [
                    "[API]",
                    "# BENCH",
                    f"HostName={node.private_ip}",
                    f"NodeId={next_node_id(node)}",
                ]
    w("config_files/config.ini", *configini)
    # Generate my.cnf (needs Node.ndb_nodeids)
    w("config_files/my.cnf",
        "[mysqld]",
        "ndbcluster",
        "user=root",
        f"basedir={WORKSPACE}/rondb",
        f"datadir={RUN_DIR}/mysqld/data",
        f"log_error={RUN_DIR}/mysqld/data/mysql-error.log",
        "log_error_verbosity=3",
        "",
        "[mysql_cluster]",
        f"ndb-connectstring={NDB_MGMD_PRI}",
      )
    for node in all_nodes():
        w(f"config_files/nodeinfo_{node.name()}",
          f"NODEINFO_ROLE={node.type}",
          f"NODEINFO_IDX={node.idx}",
          f"NODEINFO_NODEIDS=\"{' '.join(map(str, node.ndb_nodeids))}\"",
          )
    # Generate rdrs_*.json (needs Node.ndb_nodeids)
    for node in nodes_of_type("rdrs"):
        w(f"config_files/rdrs_{node.idx}.json",
          json.dumps({
              "RonDB": {
                  "Mgmds": [{"IP": NDB_MGMD_PRI}],
                  "ConnectionPoolSize": 2,
                  "NodeIDs": node.ndb_nodeids,
              },
              "REST": {
                  "NumThreads": 64,
                  "UseCompression": False,
                  "UseSingleTransaction": False,
              },
              "Rondis": {
                  "Enable": True,
                  "NumThreads": 32,
                  "NumDatabases": 2,
                  "Databases": [
                      {
                          "Index": 0,
                          "FastHCOUNT": True,
                      },
                      {
                          "Index": 1,
                          "FastHCOUNT": False,
                      },
                  ],
              },
              "Security": {
                  "APIKey": {
                      "UseHopsworksAPIKeys": False,
                  },
              },
          }, indent=4))
    # Generate prometheus.yml
    prometheus_yml = [
        "global:",
        "  scrape_interval: 15s",
        "scrape_configs:",
        "  - job_name: 'rdrs'",
        "    static_configs:",
    ]
    for node in nodes_of_type("rdrs"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:4406']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'mysqld'",
        "    static_configs:",
    ]
    for node in nodes_of_type("mysqld"):
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9104']",
            "        labels:",
            f"          instance: '{node.name()}'",
        ]
    prometheus_yml += [
        "  - job_name: 'linux'",
        "    static_configs:",
    ]
    for node in all_nodes():
        prometheus_yml += [
            f"      - targets: ['{node.private_ip}:9100']",
            "        labels:",
            f"          instance: '{node.name()}'",
            f"          instance_type: '{node.type}'",
        ]
    w("config_files/prometheus.yml", *prometheus_yml)
    # Generate grafana/
    mkdir("config_files/grafana/dashboards")
    mkdir("config_files/grafana/datasources")
    mkdir("config_files/grafana/plugins")
    mkdir("config_files/grafana/alerting")
    w("config_files/grafana/grafana.ini",
      "[paths]",
      f"data = {RUN_DIR}/grafana",
      f"logs = {RUN_DIR}/grafana/logs",
      f"provisioning = {CONFIG_FILES}/grafana",
      "[auth]",
      "disable_login_form = true",
      "[auth.anonymous]",
      "enabled = true",
      "org_role = Admin",
      )
    w("config_files/grafana/datasources/prometheus.yaml",
      "apiVersion: 1",
      "",
      "datasources:",
      "  - name: Prometheus",
      "    type: prometheus",
      "    access: proxy",
      f"    url: http://{first('prometheus').private_ip}:9090",
      "    isDefault: true",
      "    uid: prometheus",
      )
    w("config_files/grafana/dashboards/dashboards.yaml",
      "apiVersion: 1",
      "",
      "providers:",
      "  - name: 'RonDB dashboards'",
      "    orgId: 1",
      "    folder: ''",
      "    folderUid: ''",
      "    type: file",
      "    options:",
      f"      path: {CONFIG_FILES}/grafana/dashboards/dashboard.json",
      )
    Path("config_files/grafana/dashboards/dashboard.json").write_text(
        Path("dashboard.json").read_text()
    )

# Parallell processing and TUI
failed = False
def die(s):
    if s: print(red(s))
    exit(1)
def check_failed():
    if failed: die("Failed")
def stream_process(cmd, prefix, check=False):
    global failed
    def do_pipe(out, pipe_prefix):
        suppressed=0
        for line in out:
            if failed:
                suppressed += 1
            else:
                print(f"{pipe_prefix}{line.rstrip()}")
        out.close()
        if suppressed > 0:
                print(f"{pipe_prefix} {red('SUPPRESSED')} {suppressed} lines due to error elsewhere")
    proc = subprocess.Popen(
        cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    stdout_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stdout,f"{prefix} stdout: "))
    stdout_thread.start()
    stderr_thread = threading.Thread(
        target=do_pipe,
        args=(proc.stderr, f"{prefix} stderr: "))
    stderr_thread.start()
    proc.wait()
    stdout_thread.join()
    stderr_thread.join()
    if proc.returncode == 0:
        print(f"{prefix} {green('DONE')} {' '.join(cmd)}")
    else:
        print(f"{prefix} {red('EXIT')} code {proc.returncode}, command: {' '.join(cmd)}")
        failed = True
    if check: check_failed()
# Allow up to 10 concurrent rsync processes to upload config files to nodes
upload_lock = threading.Semaphore(10)
# Allow up to 2 nodes to download from repo.hops.works concurrently
repo_lock = threading.Semaphore(2)
LOCK_REPO = 1
UNLOCK_REPO = 2
LOCK_UPLOAD = 3
UNLOCK_UPLOAD = 4
def run_for_one_node(node, cmdfun):
    cmds = cmdfun(node)
    for cmd in cmds:
        if cmd == LOCK_UPLOAD: upload_lock.acquire()
        elif cmd == UNLOCK_UPLOAD: upload_lock.release()
        elif cmd == LOCK_REPO: repo_lock.acquire()
        elif cmd == UNLOCK_REPO: repo_lock.release()
        elif not failed: stream_process(cmd, node.tui_name())
def run_for_all(for_nodes, cmdfun):
    threads = [ threading.Thread(target=run_for_one_node, args=(node, cmdfun))
                for node in for_nodes ]
    for thread in threads: thread.start()
    for t in threads: t.join()
    check_failed()
def ssh_opts():
    return [
        "-i", f"{get_tf_output('key_name')}.pem",
        "-o", "StrictHostKeyChecking=no",
        "-q"]
def ssh(node, cmd=""):
    return ["ssh", *ssh_opts(), f"{USER}@{node.public_ip}",
            *([] if cmd=="" else [cmd])]
def rsync(file, node):
    return ["rsync", "-az", "--delete", "-e", f"ssh {' '.join(ssh_opts())}",
            file, f"{USER}@{node.public_ip}:"]

# install command
def do_install(for_nodes):
    generate_config_files()
    print("Deployment preview:")
    print("--------------------------------------------------")
    for node in for_nodes:
        print(f"{node.tui_name()} -> {node.private_ip} / {node.public_ip}")
    print("--------------------------------------------------")
    print("Press Ctrl+C within 3 seconds to abort.")
    time.sleep(3)
    print("Installing...")
    run_for_all(for_nodes, lambda node: [
        ssh(node, "sudo apt-get update -y"),
        ssh(node, "sudo apt-get install -y rsync libncurses6"),
        LOCK_UPLOAD,
        rsync("./scripts", node),
        rsync("./config_files", node),
        UNLOCK_UPLOAD,
        ssh(node, f"cp config_files/nodeinfo_{node.name()} config_files/nodeinfo"),
        LOCK_REPO,
        ssh(node, "bash ~/scripts/download_tarball.sh"),
        UNLOCK_REPO,
        ssh(node, "bash ~/scripts/install.sh"),
    ])

# start command
def do_start_cluster(for_nodes):
    print("Before starting services, make sure they are stopped.")
    do_stop_cluster(for_nodes)
    print_uri = False
    for start_type, _ in node_types:
        if start_type == "bench": continue
        nodes_this_iteration = [n for n in for_nodes if n.type == start_type]
        count = len(nodes_this_iteration)
        if count == 0: continue
        if start_type == "grafana": print_uri = True
        print("Waiting 2 seconds before starting services on the" +
              (f" {count} " if count>1 else " ") +
              color(start_type, first(start_type).clr, " ") +
              f" node{'s' if count>1 else ''}...")
        time.sleep(2)
        run_for_all(nodes_this_iteration, lambda node: [
            ssh(node, f"bash ~/scripts/start_{node.type}.sh")
        ])
    if print_uri:
        uri=lnk(f"http://{first('grafana').public_ip}:3000"
                "/d/rondb_bench_dashboard")
        print(f'Go to {uri} in your browser to see the dashboard.')

# deploy command
def do_deploy(for_nodes):
    do_install(for_nodes)
    do_start_cluster(for_nodes)

# stop command
def do_stop_cluster(for_nodes):
    run_for_all(for_nodes, lambda node: [
        ssh(node, "bash ~/scripts/cleanup.sh")
    ])

# open_tmux command
def do_open_tmux(for_nodes):
    def tmux(*args, check=True, **kwargs):
        cmd = ["tmux", *args]
        return subprocess.run(cmd, check=check, **kwargs).returncode == 0
    session_name = "rondb_bm"
    if not tmux("has-session", "-t", session_name, capture_output=True, check=False):
        tmux("new-session", "-d", "-s", session_name)
        for node in for_nodes:
            name = node.name()
            tmux("new-window", "-t", session_name, "-n", name, *ssh(node))
            line=f"cd {RUN_DIR}/{node.type}"
            if node.type == "bench":
                line=f"cd {RUN_DIR}; source locust/bin/activate"
            if node.type == "mysqld":
                line=f"cd {RUN_DIR}/{node.type}; alias mysql=\"{WORKSPACE}/rondb/bin/mysql -uroot\""
            tmux("send-keys", "-t", f"{session_name}:{name}", line, "C-m")
        tmux("kill-window", "-t", f"{session_name}:0")
    tmux("attach-session", "-t", session_name)

# kill_tmux command
def do_kill_tmux():
    def tmux(*args, check=True, **kwargs):
        cmd = ["tmux", *args]
        return subprocess.run(cmd, check=check, **kwargs).returncode == 0
    session_name = "rondb_bm"
    if tmux("has-session", "-t", session_name, capture_output=True, check=False):
        tmux("kill-session", "-t", session_name)

# bench locust command
def do_bench_locust(columns, rows, column_types, total_workers):
    do_start_locust(rows, total_workers, only_validate=True)
    do_populate(columns, rows, column_types)
    do_start_locust(rows, total_workers)

# populate command
def do_populate(columns, rows, column_types):
    # Validate parameters
    first('mysqld') # Assert that one exists
    if columns < 1 or 200 < columns:
        die("NUM_COLUMNS must be in the range 1 - 200")
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    if column_types not in ['INT', 'VARCHAR', 'INT_AND_VARCHAR']:
        die("COLUMN_TYPES must be one of: INT, VARCHAR, INT_AND_VARCHAR")
    batch_size = max(math.floor(10000 / columns), 1)
    column_info = {'INT': 1, 'VARCHAR': 2, 'INT_AND_VARCHAR': 0}[column_types]
    # Populate table
    run_for_one_node(first('mysqld'), lambda node: [
        ssh(node, f"bash ~/scripts/populate_locust.sh {columns} {rows}" +
            f" {batch_size} {column_info}")
    ])

# start locust command
def do_start_locust(rows, total_workers, only_validate=False):
    # Validate parameters
    if rows < 1 or 1000000000 < rows:
        die("NUM_ROWS must be in the range 1 - 1,000,000,000")
    num_bench_nodes = node_count("bench")
    cpus_per_node = get_tf_output('bench_cpus_per_node')
    cpus = num_bench_nodes * cpus_per_node
    max_workers = cpus - 1 # Reserve one CPU for master
    min_workers = max(1, num_bench_nodes - 1)
    if max_workers < min_workers:
        die("Error: Not enough CPUs for workers.")
    if total_workers == None: total_workers = max_workers
    if total_workers < min_workers or max_workers < total_workers:
        die(f"Error: Total workers must be in the range {min_workers} - {max_workers}")
    if only_validate: return
    # Stop
    do_stop_locust()
    # Start
    def cmdfun(node):
        num_workers = math.floor(total_workers / num_bench_nodes)
        if ((total_workers - num_workers * num_bench_nodes) >
            (num_bench_nodes - node.idx)):
            num_workers += 1
        return [ ssh(node, f"bash ~/scripts/start_locust.sh {rows}" +
                     f" {num_workers}") ]
    run_for_all(nodes_of_type("bench"), cmdfun)
    # Print URI
    uri=lnk(f"http://{first('bench').public_ip}:8089")
    print(f"Go to {uri} in your browser to start the benchmark")

# stop_locust command
def do_stop_locust():
    run_for_all(nodes_of_type("bench"), lambda node: [
        ssh(node, "bash ~/scripts/stop_locust.sh")
    ])

# CLI
if __name__ == "__main__":
    argc = len(sys.argv)
    node_type_names = [x[0] for x in node_types]
    if argc < 2:
        print('\n'.join([
            "Usage:",
            "  ./cluster_ctl deploy [NODE_TYPE]",
            "    Install and start RondB services.",
            "      NODE_TYPE: See bottom.",
            "",
            "  ./cluster_ctl install [NODE_TYPE]",
            "    Install RonDB services without starting. Called by deploy.",
            "      NODE_TYPE: See bottom.",
            "",
            "  ./cluster_ctl start [NODE_TYPE]",
            "    Start or restart RonDB services, without installing. Called by deploy.",
            "      NODE_TYPE: See bottom.",
            "",
            "  ./cluster_ctl stop [NODE_TYPE]",
            "    Stop RonDB services. Called by start (and hence deploy).",
            "      NODE_TYPE: See bottom.",
            "",
            "  ./cluster_ctl open_tmux [NODE_TYPE]",
            "    Attach to a tmux session with ssh connections opened to all applicable",
            "    nodes. If a session does not exist, it is created according to the NODE_TYPE",
            "    parameter, otherwise the parameter is ignored. To connect to a different set",
            "    of nodes, use kill_tmux first.",
            "      NODE_TYPE: See bottom.",
            "",
            "  ./cluster_ctl kill_tmux",
            "    Kill the tmux session created by open_tmux.",
            "",
            "  ./cluster_ctl bench_locust [--cols COLUMNS] [--rows ROWS]\\",
            "                             [--types COLUMN_TYPES] [--workers WORKERS]",
            "    Populate a table with test data and start locust ready to run a benchmark",
            "    against it.",
            "      COLUMNS: Number of columns in table. Default 100.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl populate [--cols COLUMNS] [--rows ROWS] [--types COLUMN_TYPES]",
            "    Populate a table with test data. Called by bench_locust.",
            "      COLUMNS: Number of columns in table. Default 100.",
            "      ROWS: Number of rows in table. Default 100000.",
            "      COLUMN_TYPES: The columns types in table; one of INT, VARCHAR,",
            "                    INT_AND_VARCHAR. Default INT.",
            "",
            "  ./cluster_ctl start_locust [--rows ROWS] [--workers WORKERS]\\",
            "    Start locust services. Called by bench_locust.",
            "      ROWS: Number of rows in table. Default 100000. MUST be set to the same",
            "            value as the last call to populate. Prefer using bench_locust, which",
            "            will do the right thing automatically.",
            "      WORKERS: Number of locust workers. Defaults to the maximum possible.",
            "",
            "  ./cluster_ctl stop_locust",
            "    Stop locust. Called by start_locust (and hence bench_locust).",
            "",
            "NODE_TYPE: If given, operate only on nodes of that node type, otherwise on all",
            "           nodes. Available node types are: ",
            "           "+', '.join(node_type_names),
        ]))
        die("")
    action = sys.argv[1]
    if action in ["deploy", "install", "start", "stop", "open_tmux"]:
        if argc == 2: pass
        elif argc == 3:
            node_type = sys.argv[2]
            if node_type not in node_type_names:
                die(f"Unknown node type: {node_type}. Available types: " +
                    ', '.join(node_type_names))
        else: die(f"Usage: ./cluster_ctl {action} [NODE_TYPE]")
        for_nodes = all_nodes() if argc == 2 else nodes_of_type(sys.argv[2])
        if action == "deploy": do_deploy(for_nodes)
        elif action == "install": do_install(for_nodes)
        elif action == "start": do_start_cluster(for_nodes)
        elif action == "stop": do_stop_cluster(for_nodes)
        elif action == "open_tmux": do_open_tmux(for_nodes)
        else: die("Bug")
    elif action in ["kill_tmux", "stop_locust"]:
        if argc != 2:
            die(f"Usage: ./cluster_ctl {action}")
        if action == "kill_tmux": do_kill_tmux()
        elif action == "stop_locust": do_stop_locust()
        else: die("Bug")
    elif action in ["bench_locust", "populate", "start_locust"]:
        columns = 100
        rows = 100000
        column_types = "INT"
        total_workers = None
        i = 2
        while i < argc:
            arg = sys.argv[i]
            if arg == "--cols" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --cols")
                columns = int(sys.argv[i])
            elif arg == "--rows":
                i += 1
                if i >= argc: die("Missing value after --rows")
                rows = int(sys.argv[i])
            elif arg == "--types" and action != "start_locust":
                i += 1
                if i >= argc: die("Missing value after --types")
                column_types = sys.argv[i]
            elif arg == "--workers" and action != "populate":
                i += 1
                if i >= argc: die("Missing value after --workers")
                total_workers = int(sys.argv[i])
            else:
                die(f"Unknown argument: {arg}")
            i += 1
        if action == "bench_locust":
            do_bench_locust(columns, rows, column_types, total_workers)
        elif action == "populate":
            do_populate(columns, rows, column_types)
        elif action == "start_locust":
            do_start_locust(rows, total_workers)
        else: die("Bug")
    else: die(f"Unknown action: {action}")
